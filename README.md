# olympics-data-engineering-devops
# ğŸ… Azure Data Engineering Project â€“ Paris Olympics 2024

This end-to-end Azure Data Engineering project demonstrates how to build a real-world solution for ingesting and processing data using Azure Data Factory, Azure Data Lake, and Azure Databricks â€” with CI/CD pipelines using **GitHub**.

## ğŸ“‚ Project Structure

ğŸ“ Resources
â”œâ”€â”€ Source
â”‚ â”œâ”€â”€ athletes.csv
â”‚ â”œâ”€â”€ coaches.csv
â”‚ â”œâ”€â”€ events.csv
â”‚ â””â”€â”€ NOCs.csv
â””â”€â”€ Config
â””â”€â”€ github_files_config.json

markdown
Copy
Edit

---

## âœ… Project Steps

### 1ï¸âƒ£ Azure Environment Setup

- Created a **Resource Group** to manage all related services.
- Created an **Azure Data Lake Storage Gen2** with containers:
  - `source`
  - `bronze`
- Created **Azure Data Factory (ADF)** as the orchestration tool.
- Connected ADF to **GitHub** for version control.

---

### 2ï¸âƒ£ Bronze Layer â€“ Data Ingestion (ADF)

#### Pipeline 1: `get-to-bronze`

This pipeline:
- Reads a **JSON config** file from Data Lake (`source` container) that contains GitHub file info.
- Uses ADF activities:
  - `Lookup`: Reads the config.
  - `ForEach`: Loops through each file entry.
  - `Copy Data`: Downloads CSVs from GitHub and saves as **Parquet** to the `bronze` container.

âœ… JSON Sample:
```json
[
  {
    "url": "https://raw.githubusercontent.com/anshlambagit/AzureProjectWithCICD/main/Resources/Source/coaches.csv",
    "name": "coaches"
  },
  ...
]
Pipeline 2: data-lake-injection
This pipeline:

Validates and loads NOCs.csv from the source container to bronze with rules:

File name = NOCs.csv

Column count = 5

Uses:

Get Metadata

If Condition

Copy Data
